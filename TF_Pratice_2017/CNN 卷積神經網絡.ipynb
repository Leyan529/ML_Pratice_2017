{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "LR = 0.001              # learning rate\n",
    "\n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)  # they has been normalized to range (0,1)\n",
    "test_x = mnist.test.images[:2000]\n",
    "test_y = mnist.test.labels[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQdiBJP4PeuVwkFPepnoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot one example\n",
    "print(mnist.train.images.shape)     # (55000, 28 * 28)\n",
    "print(mnist.train.labels.shape)   # (55000, 10)\n",
    "plt.imshow(mnist.train.images[0].reshape((28, 28)), cmap='gray')\n",
    "plt.title('%i' % np.argmax(mnist.train.labels[0])); plt.show()\n",
    "\n",
    "tf_x = tf.placeholder(tf.float32, [None, 28*28]) / 255.\n",
    "image = tf.reshape(tf_x, [-1, 28, 28, 1])              # (batch, height, width, channel)\n",
    "tf_y = tf.placeholder(tf.int32, [None, 10])            # input y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Merge/MergeSummary:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "'''\n",
    "tf.nn.conv2d函數是tensoflow裡面的二維的捲積函數，x是圖片的所有參數，W是此卷積層的權重，\n",
    "然後定義步長strides=[1,1,1,1]值，strides[0]和strides[3]的兩個1是默認值，\n",
    "中間兩個1代表padding時在x方向運動一步，y方向運動一步\n",
    "'''\n",
    "with tf.variable_scope('conv1'):\n",
    "    conv1 = tf.layers.conv2d(   # shape (28, 28, 1)\n",
    "        inputs=image,\n",
    "        filters=16,\n",
    "        kernel_size=5,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        activation=tf.nn.relu,\n",
    "        name='conv1'\n",
    "    )           # -> (28, 28, 16)\n",
    "\n",
    "'''\n",
    "pooling 有兩種，一種是最大值池化，一種是平均值池化，本例採用的是最大值池化tf.max_pool()。\n",
    "池化的核函數大小為2x2，因此ksize=[1,2,2,1]，步長為2，因此strides=[1,2,2,1]\n",
    "'''\n",
    "with tf.variable_scope('pool1'):\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        conv1,\n",
    "        pool_size=2,\n",
    "        strides=2,\n",
    "    )           # -> (14, 14, 16)\n",
    "    \n",
    "with tf.variable_scope('conv2'):    \n",
    "    conv2 = tf.layers.conv2d(pool1, 32, 5, 1, 'same', activation=tf.nn.relu)    # -> (14, 14, 32)\n",
    "\n",
    "with tf.variable_scope('pool2'):\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, 2, 2)    # -> (7, 7, 32)  \n",
    "\n",
    "\n",
    "with tf.variable_scope('output'):\n",
    "    flat = tf.reshape(pool2, [-1, 7*7*32])          # -> (7*7*32, )\n",
    "    output = tf.layers.dense(flat, 10)              # output layer\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=tf_y, logits=output)           # compute cost\n",
    "    tf.summary.scalar('loss', loss)     # add loss to scalar summary\n",
    "\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
    "    \n",
    "\n",
    "accuracy = tf.metrics.accuracy(          # return (acc, update_op), and create 2 local variables\n",
    "    labels=tf.argmax(tf_y, axis=1), predictions=tf.argmax(output, axis=1),)[1]\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) # the local var is for accuracy_op\n",
    "\n",
    "writer = tf.summary.FileWriter('cnn/', sess.graph)     # write to file\n",
    "merge_op = tf.summary.merge_all()                       # operation to merge all summary\n",
    "print(merge_op)\n",
    "\n",
    "sess.run(init_op)     # initialize var in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method add_summary in module tensorflow.python.summary.writer.writer:\n",
      "\n",
      "add_summary(summary, global_step=None) method of tensorflow.python.summary.writer.writer.FileWriter instance\n",
      "    Adds a `Summary` protocol buffer to the event file.\n",
      "    \n",
      "    This method wraps the provided summary in an `Event` protocol buffer\n",
      "    and adds it to the event file.\n",
      "    \n",
      "    You can pass the result of evaluating any summary op, using\n",
      "    @{tf.Session.run} or\n",
      "    @{tf.Tensor.eval}, to this\n",
      "    function. Alternatively, you can pass a `tf.Summary` protocol\n",
      "    buffer that you populate with your own data. The latter is\n",
      "    commonly done to report evaluation results in event files.\n",
      "    \n",
      "    Args:\n",
      "      summary: A `Summary` protocol buffer, optionally serialized as a string.\n",
      "      global_step: Number. Optional global step value to record with the\n",
      "        summary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Scalar\n",
    "在人工智慧當中可以使用Scalar來記錄每一次準確度、損失數值等等，最後結果類似一張折線圖。\n",
    "\n",
    "範例\n",
    "使用merge_all函數取得目前設定可視化的集合，而這裡就是tf.summary.scalar(\"count\", input)，\n",
    "最後跑迴圈並將運行summary_op寫入檔案，最後結果如下圖畫出0~100\n",
    "\n",
    "https://ithelp.ithome.com.tw/articles/10210726\n",
    "'''\n",
    "\n",
    "help(writer.add_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 | train loss: 2.2956 | test accuracy: 0.14\n",
      "Step: 50 | train loss: 0.3620 | test accuracy: 0.48\n",
      "Step: 100 | train loss: 0.2210 | test accuracy: 0.61\n",
      "Step: 150 | train loss: 0.2453 | test accuracy: 0.69\n",
      "Step: 200 | train loss: 0.1991 | test accuracy: 0.74\n",
      "Step: 250 | train loss: 0.2199 | test accuracy: 0.77\n",
      "Step: 300 | train loss: 0.0810 | test accuracy: 0.79\n",
      "Step: 350 | train loss: 0.0708 | test accuracy: 0.81\n",
      "Step: 400 | train loss: 0.1082 | test accuracy: 0.83\n",
      "Step: 450 | train loss: 0.0785 | test accuracy: 0.84\n",
      "Step: 500 | train loss: 0.1935 | test accuracy: 0.85\n",
      "Step: 550 | train loss: 0.1186 | test accuracy: 0.86\n",
      "Step: 600 | train loss: 0.1862 | test accuracy: 0.87\n",
      "Step: 650 | train loss: 0.0485 | test accuracy: 0.88\n",
      "Step: 700 | train loss: 0.0267 | test accuracy: 0.88\n",
      "Step: 750 | train loss: 0.0946 | test accuracy: 0.89\n",
      "Step: 800 | train loss: 0.0693 | test accuracy: 0.89\n",
      "Step: 850 | train loss: 0.0185 | test accuracy: 0.90\n",
      "Step: 900 | train loss: 0.1487 | test accuracy: 0.90\n",
      "Step: 950 | train loss: 0.0315 | test accuracy: 0.90\n",
      "Step: 1000 | train loss: 0.2479 | test accuracy: 0.91\n",
      "Step: 1050 | train loss: 0.0182 | test accuracy: 0.91\n",
      "Step: 1100 | train loss: 0.0135 | test accuracy: 0.91\n",
      "Step: 1150 | train loss: 0.1475 | test accuracy: 0.92\n",
      "Step: 1200 | train loss: 0.0883 | test accuracy: 0.92\n",
      "Step: 1250 | train loss: 0.0261 | test accuracy: 0.92\n",
      "Step: 1300 | train loss: 0.0339 | test accuracy: 0.92\n",
      "Step: 1350 | train loss: 0.1617 | test accuracy: 0.92\n",
      "Step: 1400 | train loss: 0.1407 | test accuracy: 0.93\n",
      "Step: 1450 | train loss: 0.0619 | test accuracy: 0.93\n",
      "Step: 1500 | train loss: 0.0505 | test accuracy: 0.93\n",
      "Step: 1550 | train loss: 0.0495 | test accuracy: 0.93\n",
      "Step: 1600 | train loss: 0.1847 | test accuracy: 0.93\n",
      "Step: 1650 | train loss: 0.0252 | test accuracy: 0.93\n",
      "Step: 1700 | train loss: 0.0629 | test accuracy: 0.93\n",
      "Step: 1750 | train loss: 0.0686 | test accuracy: 0.94\n",
      "Step: 1800 | train loss: 0.0077 | test accuracy: 0.94\n",
      "Step: 1850 | train loss: 0.0643 | test accuracy: 0.94\n",
      "Step: 1900 | train loss: 0.0050 | test accuracy: 0.94\n",
      "Step: 1950 | train loss: 0.0040 | test accuracy: 0.94\n",
      "Step: 2000 | train loss: 0.0566 | test accuracy: 0.94\n",
      "Step: 2050 | train loss: 0.0235 | test accuracy: 0.94\n",
      "Step: 2100 | train loss: 0.0206 | test accuracy: 0.94\n",
      "Step: 2150 | train loss: 0.0163 | test accuracy: 0.94\n",
      "Step: 2200 | train loss: 0.0184 | test accuracy: 0.95\n",
      "Step: 2250 | train loss: 0.0689 | test accuracy: 0.95\n",
      "Step: 2300 | train loss: 0.0045 | test accuracy: 0.95\n",
      "Step: 2350 | train loss: 0.0216 | test accuracy: 0.95\n",
      "Step: 2400 | train loss: 0.1019 | test accuracy: 0.95\n",
      "Step: 2450 | train loss: 0.0044 | test accuracy: 0.95\n",
      "Step: 2500 | train loss: 0.0225 | test accuracy: 0.95\n",
      "Step: 2550 | train loss: 0.0664 | test accuracy: 0.95\n",
      "Step: 2600 | train loss: 0.0420 | test accuracy: 0.95\n",
      "Step: 2650 | train loss: 0.0012 | test accuracy: 0.95\n",
      "Step: 2700 | train loss: 0.0378 | test accuracy: 0.95\n",
      "Step: 2750 | train loss: 0.0644 | test accuracy: 0.95\n",
      "Step: 2800 | train loss: 0.0019 | test accuracy: 0.95\n",
      "Step: 2850 | train loss: 0.1693 | test accuracy: 0.95\n",
      "Step: 2900 | train loss: 0.0080 | test accuracy: 0.95\n",
      "Step: 2950 | train loss: 0.0375 | test accuracy: 0.95\n",
      "Step: 3000 | train loss: 0.0053 | test accuracy: 0.96\n",
      "Step: 3050 | train loss: 0.0035 | test accuracy: 0.96\n",
      "Step: 3100 | train loss: 0.0153 | test accuracy: 0.96\n",
      "Step: 3150 | train loss: 0.0200 | test accuracy: 0.96\n",
      "Step: 3200 | train loss: 0.0500 | test accuracy: 0.96\n",
      "Step: 3250 | train loss: 0.1518 | test accuracy: 0.96\n",
      "Step: 3300 | train loss: 0.0141 | test accuracy: 0.96\n",
      "Step: 3350 | train loss: 0.0101 | test accuracy: 0.96\n",
      "Step: 3400 | train loss: 0.0679 | test accuracy: 0.96\n",
      "Step: 3450 | train loss: 0.0048 | test accuracy: 0.96\n",
      "Step: 3500 | train loss: 0.0527 | test accuracy: 0.96\n",
      "Step: 3550 | train loss: 0.1422 | test accuracy: 0.96\n",
      "Step: 3600 | train loss: 0.0041 | test accuracy: 0.96\n",
      "Step: 3650 | train loss: 0.0727 | test accuracy: 0.96\n",
      "Step: 3700 | train loss: 0.0137 | test accuracy: 0.96\n",
      "Step: 3750 | train loss: 0.0013 | test accuracy: 0.96\n",
      "Step: 3800 | train loss: 0.0555 | test accuracy: 0.96\n",
      "Step: 3850 | train loss: 0.0357 | test accuracy: 0.96\n",
      "Step: 3900 | train loss: 0.0031 | test accuracy: 0.96\n",
      "Step: 3950 | train loss: 0.0043 | test accuracy: 0.96\n",
      "Step: 4000 | train loss: 0.1309 | test accuracy: 0.96\n",
      "Step: 4050 | train loss: 0.0189 | test accuracy: 0.96\n",
      "Step: 4100 | train loss: 0.0206 | test accuracy: 0.96\n",
      "Step: 4150 | train loss: 0.0063 | test accuracy: 0.96\n",
      "Step: 4200 | train loss: 0.0083 | test accuracy: 0.96\n",
      "Step: 4250 | train loss: 0.0848 | test accuracy: 0.96\n",
      "Step: 4300 | train loss: 0.0041 | test accuracy: 0.96\n",
      "Step: 4350 | train loss: 0.1359 | test accuracy: 0.96\n",
      "Step: 4400 | train loss: 0.0099 | test accuracy: 0.96\n",
      "Step: 4450 | train loss: 0.0039 | test accuracy: 0.96\n",
      "Step: 4500 | train loss: 0.0051 | test accuracy: 0.97\n",
      "Step: 4550 | train loss: 0.0909 | test accuracy: 0.97\n",
      "Step: 4600 | train loss: 0.0137 | test accuracy: 0.97\n",
      "Step: 4650 | train loss: 0.0673 | test accuracy: 0.97\n",
      "Step: 4700 | train loss: 0.0210 | test accuracy: 0.97\n",
      "Step: 4750 | train loss: 0.0044 | test accuracy: 0.97\n",
      "Step: 4800 | train loss: 0.0009 | test accuracy: 0.97\n",
      "Step: 4850 | train loss: 0.0195 | test accuracy: 0.97\n",
      "Step: 4900 | train loss: 0.0668 | test accuracy: 0.97\n",
      "Step: 4950 | train loss: 0.0114 | test accuracy: 0.97\n",
      "Step: 5000 | train loss: 0.0022 | test accuracy: 0.97\n",
      "Step: 5050 | train loss: 0.1148 | test accuracy: 0.97\n",
      "Step: 5100 | train loss: 0.0452 | test accuracy: 0.97\n",
      "Step: 5150 | train loss: 0.0156 | test accuracy: 0.97\n",
      "Step: 5200 | train loss: 0.0146 | test accuracy: 0.97\n",
      "Step: 5250 | train loss: 0.0121 | test accuracy: 0.97\n",
      "Step: 5300 | train loss: 0.0306 | test accuracy: 0.97\n",
      "Step: 5350 | train loss: 0.0348 | test accuracy: 0.97\n",
      "Step: 5400 | train loss: 0.0124 | test accuracy: 0.97\n",
      "Step: 5450 | train loss: 0.0119 | test accuracy: 0.97\n",
      "Step: 5500 | train loss: 0.1015 | test accuracy: 0.97\n",
      "Step: 5550 | train loss: 0.0011 | test accuracy: 0.97\n",
      "Step: 5600 | train loss: 0.0018 | test accuracy: 0.97\n",
      "Step: 5650 | train loss: 0.0008 | test accuracy: 0.97\n",
      "Step: 5700 | train loss: 0.0125 | test accuracy: 0.97\n",
      "Step: 5750 | train loss: 0.0015 | test accuracy: 0.97\n",
      "Step: 5800 | train loss: 0.0458 | test accuracy: 0.97\n",
      "Step: 5850 | train loss: 0.0618 | test accuracy: 0.97\n",
      "Step: 5900 | train loss: 0.0279 | test accuracy: 0.97\n",
      "Step: 5950 | train loss: 0.0068 | test accuracy: 0.97\n",
      "Step: 6000 | train loss: 0.0167 | test accuracy: 0.97\n",
      "Step: 6050 | train loss: 0.0111 | test accuracy: 0.97\n",
      "Step: 6100 | train loss: 0.0576 | test accuracy: 0.97\n",
      "Step: 6150 | train loss: 0.1029 | test accuracy: 0.97\n",
      "Step: 6200 | train loss: 0.0438 | test accuracy: 0.97\n",
      "Step: 6250 | train loss: 0.0232 | test accuracy: 0.97\n",
      "Step: 6300 | train loss: 0.0842 | test accuracy: 0.97\n",
      "Step: 6350 | train loss: 0.0020 | test accuracy: 0.97\n",
      "Step: 6400 | train loss: 0.0519 | test accuracy: 0.97\n",
      "Step: 6450 | train loss: 0.0022 | test accuracy: 0.97\n",
      "Step: 6500 | train loss: 0.0043 | test accuracy: 0.97\n",
      "Step: 6550 | train loss: 0.0179 | test accuracy: 0.97\n",
      "Step: 6600 | train loss: 0.0002 | test accuracy: 0.97\n",
      "Step: 6650 | train loss: 0.0202 | test accuracy: 0.97\n",
      "Step: 6700 | train loss: 0.0088 | test accuracy: 0.97\n",
      "Step: 6750 | train loss: 0.0063 | test accuracy: 0.97\n",
      "Step: 6800 | train loss: 0.0155 | test accuracy: 0.97\n",
      "Step: 6850 | train loss: 0.0013 | test accuracy: 0.97\n",
      "Step: 6900 | train loss: 0.0023 | test accuracy: 0.97\n",
      "Step: 6950 | train loss: 0.0054 | test accuracy: 0.97\n",
      "Step: 7000 | train loss: 0.0071 | test accuracy: 0.97\n",
      "Step: 7050 | train loss: 0.0044 | test accuracy: 0.97\n",
      "Step: 7100 | train loss: 0.0069 | test accuracy: 0.97\n",
      "Step: 7150 | train loss: 0.0269 | test accuracy: 0.97\n",
      "Step: 7200 | train loss: 0.0178 | test accuracy: 0.97\n",
      "Step: 7250 | train loss: 0.0140 | test accuracy: 0.97\n",
      "Step: 7300 | train loss: 0.0050 | test accuracy: 0.97\n",
      "Step: 7350 | train loss: 0.0046 | test accuracy: 0.97\n",
      "Step: 7400 | train loss: 0.0033 | test accuracy: 0.97\n",
      "Step: 7450 | train loss: 0.0029 | test accuracy: 0.97\n",
      "Step: 7500 | train loss: 0.0250 | test accuracy: 0.97\n",
      "Step: 7550 | train loss: 0.0005 | test accuracy: 0.97\n",
      "Step: 7600 | train loss: 0.0012 | test accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7650 | train loss: 0.0015 | test accuracy: 0.97\n",
      "Step: 7700 | train loss: 0.0084 | test accuracy: 0.97\n",
      "Step: 7750 | train loss: 0.0009 | test accuracy: 0.97\n",
      "Step: 7800 | train loss: 0.0016 | test accuracy: 0.97\n",
      "Step: 7850 | train loss: 0.0008 | test accuracy: 0.97\n",
      "Step: 7900 | train loss: 0.0013 | test accuracy: 0.97\n",
      "Step: 7950 | train loss: 0.0021 | test accuracy: 0.97\n",
      "Step: 8000 | train loss: 0.0034 | test accuracy: 0.97\n",
      "Step: 8050 | train loss: 0.0024 | test accuracy: 0.97\n",
      "Step: 8100 | train loss: 0.0189 | test accuracy: 0.97\n",
      "Step: 8150 | train loss: 0.0145 | test accuracy: 0.97\n",
      "Step: 8200 | train loss: 0.0025 | test accuracy: 0.97\n",
      "Step: 8250 | train loss: 0.0002 | test accuracy: 0.97\n",
      "Step: 8300 | train loss: 0.2152 | test accuracy: 0.97\n",
      "Step: 8350 | train loss: 0.0002 | test accuracy: 0.98\n",
      "Step: 8400 | train loss: 0.0040 | test accuracy: 0.98\n",
      "Step: 8450 | train loss: 0.0058 | test accuracy: 0.98\n",
      "Step: 8500 | train loss: 0.0228 | test accuracy: 0.98\n",
      "Step: 8550 | train loss: 0.0170 | test accuracy: 0.98\n",
      "Step: 8600 | train loss: 0.0120 | test accuracy: 0.98\n",
      "Step: 8650 | train loss: 0.0138 | test accuracy: 0.98\n",
      "Step: 8700 | train loss: 0.0529 | test accuracy: 0.98\n",
      "Step: 8750 | train loss: 0.0022 | test accuracy: 0.98\n",
      "Step: 8800 | train loss: 0.0042 | test accuracy: 0.98\n",
      "Step: 8850 | train loss: 0.0027 | test accuracy: 0.98\n",
      "Step: 8900 | train loss: 0.0221 | test accuracy: 0.98\n",
      "Step: 8950 | train loss: 0.0020 | test accuracy: 0.98\n",
      "Step: 9000 | train loss: 0.0022 | test accuracy: 0.98\n",
      "Step: 9050 | train loss: 0.0016 | test accuracy: 0.98\n",
      "Step: 9100 | train loss: 0.0008 | test accuracy: 0.98\n",
      "Step: 9150 | train loss: 0.0009 | test accuracy: 0.98\n",
      "Step: 9200 | train loss: 0.0007 | test accuracy: 0.98\n",
      "Step: 9250 | train loss: 0.0436 | test accuracy: 0.98\n",
      "Step: 9300 | train loss: 0.0013 | test accuracy: 0.98\n",
      "Step: 9350 | train loss: 0.0029 | test accuracy: 0.98\n",
      "Step: 9400 | train loss: 0.0006 | test accuracy: 0.98\n",
      "Step: 9450 | train loss: 0.0068 | test accuracy: 0.98\n",
      "Step: 9500 | train loss: 0.0007 | test accuracy: 0.98\n",
      "Step: 9550 | train loss: 0.0130 | test accuracy: 0.98\n",
      "Step: 9600 | train loss: 0.0727 | test accuracy: 0.98\n",
      "Step: 9650 | train loss: 0.0170 | test accuracy: 0.98\n",
      "Step: 9700 | train loss: 0.0002 | test accuracy: 0.98\n",
      "Step: 9750 | train loss: 0.0052 | test accuracy: 0.98\n",
      "Step: 9800 | train loss: 0.0018 | test accuracy: 0.98\n",
      "Step: 9850 | train loss: 0.0169 | test accuracy: 0.98\n",
      "Step: 9900 | train loss: 0.0043 | test accuracy: 0.98\n",
      "Step: 9950 | train loss: 0.0325 | test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()  # define a saver for saving and restoring\n",
    "save_path = 'cnn/model.ckpt'\n",
    "for step in range(10000):\n",
    "    b_x, b_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "    _,loss_ = sess.run([train_op,loss], {tf_x: b_x, tf_y: b_y}) \n",
    "    result = sess.run(merge_op, {tf_x: b_x, tf_y: b_y}) \n",
    "    writer.add_summary(result, step)\n",
    "    if step % 50 == 0:\n",
    "        saver.save(sess, save_path, global_step=step)\n",
    "        accuracy_, flat_representation = sess.run([accuracy, flat], {tf_x: test_x, tf_y: test_y})\n",
    "        print('Step:', step, '| train loss: %.4f' % loss_, '| test accuracy: %.2f' % accuracy_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Saver 保存讀取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save():  \n",
    "    saver = tf.train.Saver()  # define a saver for saving and restoring\n",
    "    saver.save(sess, save_path)  # meta_graph is not recommended       \n",
    "\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is reload\n",
      "Found checkpoint, try to restore...\n",
      "INFO:tensorflow:Restoring parameters from cnn\\model.ckpt\n",
      "<tf.Variable 'conv1/conv1/kernel:0' shape=(5, 5, 1, 16) dtype=float32_ref>\n",
      "<tf.Variable 'conv1/conv1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'conv2/conv2d/kernel:0' shape=(5, 5, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv2/conv2d/bias:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'output/dense/kernel:0' shape=(1568, 10) dtype=float32_ref>\n",
      "<tf.Variable 'output/dense/bias:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# destroy previous net\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def reload():\n",
    "    print('This is reload')\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname(save_path))       \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "#             print(tf.train.latest_checkpoint('cnn'))            \n",
    "            print('Found checkpoint, try to restore...')\n",
    "            saver = tf.train.import_meta_graph(''.join([ckpt.model_checkpoint_path, '.meta']))   \n",
    "            saver.restore(sess, tf.train.latest_checkpoint(save_path.replace('model.ckpt','')))\n",
    "\n",
    "            # 列印出網路中可訓練的權重引數名\n",
    "            for var in tf.trainable_variables():\n",
    "                print(var)           \n",
    "reload() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload 後繼續train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 | train loss: 0.0003 | test accuracy: 0.98\n",
      "Step: 50 | train loss: 0.0055 | test accuracy: 0.98\n",
      "Step: 100 | train loss: 0.0008 | test accuracy: 0.98\n",
      "Step: 150 | train loss: 0.0001 | test accuracy: 0.98\n",
      "Step: 200 | train loss: 0.0015 | test accuracy: 0.98\n",
      "Step: 250 | train loss: 0.0002 | test accuracy: 0.98\n",
      "Step: 300 | train loss: 0.0199 | test accuracy: 0.98\n",
      "Step: 350 | train loss: 0.0071 | test accuracy: 0.98\n",
      "Step: 400 | train loss: 0.0452 | test accuracy: 0.98\n",
      "Step: 450 | train loss: 0.0018 | test accuracy: 0.98\n",
      "Step: 500 | train loss: 0.0006 | test accuracy: 0.98\n",
      "Step: 550 | train loss: 0.0027 | test accuracy: 0.98\n",
      "Step: 600 | train loss: 0.0038 | test accuracy: 0.98\n",
      "Step: 650 | train loss: 0.0037 | test accuracy: 0.98\n",
      "Step: 700 | train loss: 0.0053 | test accuracy: 0.98\n",
      "Step: 750 | train loss: 0.0010 | test accuracy: 0.98\n",
      "Step: 800 | train loss: 0.0002 | test accuracy: 0.98\n",
      "Step: 850 | train loss: 0.0026 | test accuracy: 0.98\n",
      "Step: 900 | train loss: 0.0002 | test accuracy: 0.98\n",
      "Step: 950 | train loss: 0.0003 | test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "    b_x, b_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "    _,loss_ = sess.run([train_op,loss], {tf_x: b_x, tf_y: b_y}) \n",
    "    result = sess.run(merge_op, {tf_x: b_x, tf_y: b_y}) \n",
    "    writer.add_summary(result, step)\n",
    "    if step % 50 == 0:\n",
    "        saver.save(sess, save_path, global_step=step)\n",
    "        accuracy_, flat_representation = sess.run([accuracy, flat], {tf_x: test_x, tf_y: test_y})\n",
    "        print('Step:', step, '| train loss: %.4f' % loss_, '| test accuracy: %.2f' % accuracy_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
